# Modern data science with r Ch.12 Simulation
### https://beanumber.github.io/mdsr2e/ch-simulation.html
set.seed(20200918)

# Extended example: Grouping cancers
### Before concluding that there are no relationships, it’s helpful to rule out some other possibilities. Perhaps the data reduction and data interpretation methods you used are not powerful enough. Another set of methods might be better. Or perhaps there isn’t enough data to be able to detect the patterns you are looking for. Simulations can help here. 
library(tidyverse)
library(mdsr)
NCI60 <- etl_NCI60()
spreads <- NCI60 %>%
  pivot_longer(
    -Probe, values_to = "expression", 
    names_to = "cellLine"
  ) %>%  
  group_by(Probe) %>%
  summarize(N = n(), spread = sd(expression)) %>%
  arrange(desc(spread)) %>%
  mutate(order = row_number())

### to check data preprocessing
 forcheck  <- NCI60 %>%
  pivot_longer(
    -Probe, values_to = "expression", 
    names_to = "cellLine"
  )

dim(NCI60)
dim(forcheck)
head(NCI60)
head(forcheck)
dim(spreads)
head(spreads)

### simulation with probe
library(mosaic)
sim_spreads <- NCI60 %>%
  pivot_longer(
    -Probe, values_to = "expression", 
    names_to = "cellLine"
  ) %>%
  mutate(Probe = mosaic::shuffle(Probe)) %>%
  group_by(Probe) %>%
  summarize(N = n(), spread = sd(expression)) %>%
  arrange(desc(spread)) %>%
  mutate(order = row_number())

### What makes this a simulation is the fourth line of the expression where we call shuffle(). In that line, we replace each of the probe labels with a randomly selected label. The result is that the expression has been statistically disconnected from any other variable, particularly cellLine. The simulation creates the kind of data that would result from a system in which the probe expression data is meaningless. In other words, the simulation mechanism matches the null hypothesis that the probe labels are irrelevant. By comparing the real NCI60 data to the simulated data, we can see which probes give evidence that the null hypothesis is false. Let’s compare the top 500 spread values in spreads and sim_spreads.

spreads %>%
  filter(order <= 500) %>%
  ggplot(aes(x = order, y = spread)) +
  geom_line(color = "blue", size = 2) +
  geom_line(
    data = filter(sim_spreads, order <= 500), 
    color = "red", 
    size = 2
  )

### We can tell a lot from the results of the simulation shown in Figure 12.1. If we decided to use the top 500 probes, we would risk including many that were no more variable than random noise (i.e., that which could have been generated under the null hypothesis).

### But if we set the threshold much lower, including, say, only those probes with a spread greater than 5.0, we would be unlikely to include any that were generated by a mechanism consistent with the null hypothesis. The simulation is telling us that it would be good to look at roughly the top 50 probes, since that is about how many in NCI60 were out of the range of the simulated results for the null hypothesis. Methods of this sort are often identified as false discovery rate methods.

# randomizing functions
### The workhorse of simulation is the generation of random numbers in the range from zero to one, with each possibility being equally likely. In R, the most widely used such uniform random number generator is runif(). ... Other randomization devices can be built out of uniform random number generators. To illustrate, here is a device for selecting one value at random from a vector:

select_one <- function(vec) {
  n <- length(vec)
  ind <- which.max(runif(n))
  vec[ind]
}
select_one(letters) # letters are a, b, c, ..., z

### The select_one() function is functionally equivalent to sample_n() with the size argument set to 1.

### resample() is equivalent to sample() with the replace argument set to TRUE, while shuffle() is equivalent to sample() with size equal to the number of rows in the data frame and replace equal to FALSE. Non-uniform sampling can be achieved using the prob argument.

# Simulating variability
## the partial planned rendezvous
### Imagine a situation where Sally and Joan plan to meet to study in their college campus center Mosteller (1987). They are both impatient people who will wait only ten minutes for the other before leaving. But their planning was incomplete. Sally said, “Meet me between 7 and 8 tonight at the center.” 

n <- 100000
sim_meet <- tibble(
  sally = runif(n, min = 0, max = 60),
  joan = runif(n, min = 0, max = 60)
) %>%
  mutate(
    result = ifelse(abs(sally - joan) <= 10, "They meet", "They do not")
  )
mosaic::tally(~ result, format = "percent", data = sim_meet)
mosaic::binom.test(~result, n, success = "They meet", data = sim_meet)

ggplot(data = sim_meet, aes(x = joan, y = sally, color = result)) + 
  geom_point(alpha = 0.3) + 
  geom_abline(intercept = 10, slope = 1) + 
  geom_abline(intercept = -10, slope = 1)

##  The jobs report
jobs_true <- 150
jobs_se <- 65  # in thousands of jobs
gen_samp <- function(true_mean, true_sd, 
                     num_months = 12, delta = 0, id = 1) {
  samp_year <- rep(true_mean, num_months) + 
    rnorm(num_months, mean = delta * (1:num_months), sd = true_sd)
  return(
    tibble(
      jobs_number = samp_year, 
      month = as.factor(1:num_months), 
      id = id
    )
  )
}

n_sims <- 3
params <- tibble(
  sd = c(0, rep(jobs_se, n_sims)), 
  id = c("Truth", paste("Sample", 1:n_sims))
)
?paste()
params

### Finally, we will actually perform the simulation using the pamp_dfr() function from the purrr package
library(purrr)
df <- params %>%
  pmap_dfr(~gen_samp(true_mean = jobs_true, true_sd = ..1, id = ..2))

ggplot(data = df, aes(x = month, y = jobs_number)) + 
  geom_hline(yintercept = jobs_true, linetype = 2) + 
  geom_col() + 
  facet_wrap(~ id) + ylab("Number of new jobs (in thousands)")

### While all of the three samples are taken from a “true” universe where the jobs number is constant, each could easily be misinterpreted to conclude that the numbers of new jobs was decreasing at some point during the series. The moral is clear: It is important to be able to understand the underlying variability of a system before making inferential conclusions.

## Restaurant health and sanitation grades
### Health inspectors make unannounced inspections at least once per year to each restaurant. Those with a score between 0 and 13 points receive a coveted A grade, those with 14 to 27 points receive the less desirable B, and those of 28 or above receive a C.
minval <- 7
maxval <- 19
JustScores <- Violations %>%
  filter(score >= minval & score <= maxval) %>%
  select(dba, score) %>%
  distinct()
?distinct() # Select only unique/distinct rows from a data frame. This is similar to ‘unique.data.frame()’ but considerably faster.
ggplot(data = JustScores, aes(x = score)) + 
  geom_histogram(binwidth = 0.5) + 
  geom_vline(xintercept = 13, linetype = 2) + 
  scale_x_continuous(breaks = minval:maxval) + 
  annotate("text", x = 10.5, y = 10300, label = "A grade: score of 13 or less")

### Let’s carry out a simple simulation in which a grade of 13 or 14 is equally likely. The nflip() function allows us to flip a fair coin that determines whether a grade is a 14 (heads) or 13 (tails).

scores <- mosaic::tally(~score, data = JustScores)
scores
obs_diff <- scores["13"] - scores["14"]
mean(scores[c("13", "14")])
random_flip <- 1:1000 %>%
  map_dbl(~mosaic::nflip(scores["13"] + scores["14"])) %>%
  enframe(name = "sim", value = "heads")
head(random_flip, 3)
?map_dbl

ggplot(data = random_flip, aes(x = heads)) + 
  geom_histogram(binwidth = 5) + 
  xlim(c(2100, NA)) + 
  geom_vline(xintercept = scores["14"], col = "red") + 
  annotate("text", x = 2137, y = 45, label = "observed", hjust = "left") + 
  xlab("Number of restaurants with scores of 14 (if equal probability)") 

## Simulating a complex system
any_active <- function(df) {
  # return TRUE if someone has not finished
  return(max(df$endtime) == Inf)
}

next_customer <- function(df) {
  # returns the next customer in line
  res <- df %>%
    filter(endtime == Inf) %>%
    arrange(arrival)
  return(head(res, 1))
}

update_customer <- function(df, cust_num, end_time) {
  # sets the end time of a specific customer
  return(
    df %>%
      mutate(
        endtime = ifelse(custnum == cust_num, end_time, endtime)
      )
  )
}

### We will assume that the number of customers follows a Poisson distribution (useful for modeling counts) and the transaction times follow an exponential distribution (long right tail with most transactions happening quickly but with some transactions taking a long time).

run_sim <- function(n = 1/2, m = 3/2, hours = 6) {
# simulation of bank where there is just one teller
# n: expected number of customers per minute
# m: expected length of transaction is m minutes
# hours: bank open for this many hours
  customers <- rpois(hours * 60, lambda = n)
  arrival <- numeric(sum(customers))
  position <- 1
  for (i in 1:length(customers)) {
    numcust <- customers[i]
    if (numcust != 0) {
      arrival[position:(position + numcust - 1)] <- rep(i, numcust)
      position <- position + numcust
    }
  }
  duration <- rexp(length(arrival), rate = 1/m)   # E[X]=m
  df <- tibble(
    arrival, 
    duration, 
    custnum = 1:length(duration), 
    endtime = Inf
  )

  endtime <- 0 # set up beginning of simulation
  while (any_active(df)) { # anyone left to serve?
    next_one <- next_customer(df)
    now <- ifelse(next_one$arrival >= endtime, next_one$arrival, endtime)
    endtime <- now + next_one$duration
    df <- update_customer(df, next_one$custnum, endtime)
  }
  df <- mutate(df, totaltime = endtime - arrival)
  return(ggformula::df_stats(~ totaltime, data = df))
}

sim_results <- 1:3 %>%
  map_dfr(~run_sim())
sim_results

### Sampling variability is inherent in simulations: Our results will be sensitive to the number of computations that we are willing to carry out. 
campus_sim <- function(sims = 1000, wait = 10) {
  sally <- runif(sims, min = 0, max = 60)
  joan <- runif(sims, min = 0, max = 60)
  return(
    tibble(
      num_sims = sims, 
      meet = sum(abs(sally - joan) <= wait),
      meet_pct = meet / num_sims,
    )
  )
}

reps <- 5000
sim_results <- 1:reps %>%
  map_dfr(~map_dfr(c(100, 400, 1600), campus_sim))

sim_results %>%
  group_by(num_sims) %>%
  skim(meet_pct)

### Note that each of the simulations yields an unbiased estimate of the true probability that they meet, but there is variability within each individual simulation (of size 100, 400, or 1600). The standard deviation is halved each time we increase the number of simulations by a factor of 4. We can display the results graphically
ggplot(data = sim_results, aes(x = meet_pct, color = factor(num_sims))) + 
  geom_density(size = 2) + 
  scale_x_continuous("Proportion of times that Sally and Joan meet")

### What would be a reasonable value for num_sims in this setting? The answer depends on how accurate we want to be. (And we can also simulate to see how variable our results are!) Carrying out 20,000 simulations yields relatively little variability and would likely be sufficient for a first pass. We could state that these results have converged sufficiently close to the true value since the sampling variability due to the simulation is negligible.

1:reps %>%
  map_dfr(~campus_sim(20000)) %>%
  group_by(num_sims) %>%
  skim(meet_pct)

# Further resource
### Rizzo (2019) provides a comprehensive introduction to statistical computing in R, while Horton, Brown, and Qian (2004) and Horton (2013) describe the use of R for simulation studies. The importance of simulation as part of an analyst’s toolbox is enunciated in American Statistical Association Undergraduate Guidelines Workgroup (2014), N. J. Horton (2015), and National Academies of Science, Engineering, and Medicine (2018). The simstudy package can be used to simplify data generation or exploration using simulation.